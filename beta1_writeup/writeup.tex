\documentclass[10pt,twosidep]{article}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{verbatim}
\usepackage{indentfirst}
\usepackage{syntonly}
\usepackage{fancyhdr}
\usepackage{graphicx}
\usepackage{enumitem}
\usepackage{amsthm}

\usepackage[top = 1.2in, bottom = 1.2in, left = 1.3in, right = 1.3in]{geometry}
%\usepackage{xcolor}
%\usepackage{listings}
%\usepackage{minted}
\newtheorem{lemma}{Lemma}

\begin{document}
\pagestyle{fancy}
%\definecolor{bg}{RGB}{30, 30, 30}
%\newminted[C++]{cpp}{mathescape, numbersep = 5pt, obeytabs, gobble = 10, frame = single, framesep = -2mm, bgcolor = bg}

\setlength{\parindent}{2em}
\setlength{\footskip}{30pt}
\setlength{\baselineskip}{1.3\baselineskip}

\title{Beta I Writeup}
\author{Yuzhou Gu, Haoran Xu, Yinzhan Xu, Chengkai Zhang}
\maketitle{}

\section{Profiling Data}

scout\_search, eval, move\_gen

\section{The changes so far}

\begin{enumerate}
	\item We used a uint64\_t to store the cells on board that are lasered. This replacement saves some scans of the whole board, and also saved some memory space. Also, in eval.c, the laser was computed several times on the same board; since we are using a bitmap, we can simply use a bitmap to store the laser and use this bitmap for all the computation. It gives about $20\%$ speedup.

	\item We also use a bitmap to store the cells on the board that are white pieces and another bitmap to store the cells that are black pieces (it is supplementary and the original representation is still stored). This helps to reduce some blind scan of the whole board.  It gives about $15\%$ speedup.

	\item We change ARR\_WIDTH from 16 to 10. Therefore ARR\_SIZE decreases from 256 to 100. This gives about $30\%$ speedup.
	\item We used some constant tables to reduce work. 
	The pcentral function in eval.c repeats calculation a lot of times. We precompute the results and stores the result in a constant table.
	We use constant table to remove many divisions in the code.
	These optimizations give about $10\%$ speedup.
	\item We changed small functions to inline functions or macros. This gives about $20\%$ speedup.
	\item We found that in some places, it is unnecessary to use int. We replaced them with appropriate smaller types such as uint8\_t and uint16\_t. This gives about $10\%$ speedup.

	\item In scoutsearch function, there is a incremental search function called. We found that this function is very slow so we decide to replace it with a more efficient sorting algorithm. First we tried quick sort but that does not help. Then we discover that mostly only the smallest several items will be used, so we decide to find the smallest element each time with the iteration. Using bruteforce to find it does not make the code faster, so we instead maintained a range tree to maintain the smallest element in the array. It gives about $10\%$ speedup. 
	
	\item subpv in .. is only used to store the best moves up the search depth, but we really only need the first move. Thus, we decide to delete the array and replace it with a variable. This saves the memory and thus improves the speed. It gives about $20\%$ speedup. 
	
	\item We modified some logic in eval.c but keeping the result as the same as the original. For example, we merged the case for king and for pawn in the switch struct, and then minus score for king out of the loop. Such improvements give a $20\%$ speedup. 	
\end{enumerate}

\section{Generating CloseBook}
	As suggested in the leiserchess document, we generated the closebook 
	for cases where the total number of pawns is no more than one. 
	There are 64 positions on the chessboard and each piece has 4 directions, 
	so there are 256 possibilities for each piece. 
	Two kings and a pawn gives us about $256^3*2\approx 3.2\times 10^7$ possible chessboard states 
	(the pawn may belong to either side).
	Our goal is to calculate precisely which states are winning states and which states are losing states.
	The huge data scale, not acyclic transition graph and KO rule 
	(which limits possible moves according to the previous move) complicate the problem.
	Below we will explain our solution to those complications. 

	\begin{itemize}
	\item KO rule: KO rule prevents a player to swap back two pieces that are just swapped by the opponent 
	if no pieces are zagged. Since there are only 3 pieces on the board, only 3 swap moves are possible. 
	Therefore, we add into our chessboard state an extra variable, denoting if the previous move was a swap move, 
	and the type of swap move. This enlarges the number of possible states to $256^3*2*4$.
	\item Storing the transition graph: we compressed a board state into an int, which takes 4 bytes. 
	There are 17 possible moves on average, so it takes about 8GB memory to store the graph. 
	\item To determine which states are winning states, since the graph contains cycles, 
	we cannot simply DFS it like what we do for decision trees. Instead, 
	we need to do it in the other direction. 
	We first figure out the nodes that can be won/will must lose in the next move, 
	record the result for those nodes, and put them into a queue. 
	Every time we pick out a node and remove the node from graph. 
	If the node is a losing node, we mark all nodes that can reach it in one move as winning and put them into queue. 
	If all moves of a node lead to a winning node, we mark this node as losing and put it into queue. 
	The nodes that are not determined in the process are draw nodes. 
	\end{itemize}
	Note that KO rule does determines which opponent wins in some states. In that case 
	we just record those states as "uncalculated", because the number of such cases are very small.
	So to conclude, there are 4 possiblities for each state (win, lose, draw, determined by KO rule), 
	which can be recorded in 2 bits. The total size of the output table
	is $256^3*2/4=8$ Megabytes, which is acceptable.
	
\end{document}
